#+TITLE: Log

This is a research project, so let's take notes. This file is just a
chronological dump of problems I face and ideas I have.

* Notes
** [2023-04-05 Wed 13:22] Tokenising
   - Note taken on [2023-04-12 Wed 12:23] \\
     CxxWrap has built in functions for converting to libstd types, strings and
     vectors specifically. So I've changed the C++ code to use vectors and all of the
     conversions happen on the Julia side. I only need to convert explicitely when
     decoding, but I'm going to assume there's a conversion happening on encoding as
     well.
   Julia has a package WordTokenizers.jl which has a tokeniser whose docs say
   "basically the Sentencepiece processor's re-implementation in julia." Not the
   most promising lead, but better than calling out via FFI if it works.

   Let's construct a test to see if it works.

   Quick failure. The model I have is in a binary format which I don't know how
   to parse. WordTokenizers' implementation requires a julia dict which I can't
   make for it with tools I have.

   Next idea: let's just call sentencepiece via FFI.

   Source: https://github.com/google/sentencepiece

   =llama/llama/tokenizer.py= ought to include all of the config params we need.

   [[https://github.com/google/sentencepiece/blob/master/src/sentencepiece_processor.h][Example usage]]:

 #+BEGIN_SRC c
   Usage:
     SentencePieceProcessor sp;
     sp.Load("//path/to/model");

     vector<string> sps;
     sp.Encode("hello world.", &sps).IgnoreError();

     vector<int> ids;
     sp.Encode("hello world.", &ids).IgnoreError();

     string detok;
     sp.Decode(sps, &detok);
     CHECK_EQ("hello world.", detok).IgnoreError();

     sp.Decode(ids, &detok);
     CHECK_EQ("hello world.", detok).IgnoreError();

     // We can also use SentencePieceText which manages the byte-offsets
     // between user input (output) and internal sentence pieces.

     SentencePieceText spt;
     sp.Encode("hello world.", &spt);
     // Emits the byte range of each piece.
     for (const auto &piece : spt.pieces()) {
        LOG(INFO) << piece.begin() << " " << piece.end();
     }

     sp.Decode({0, 1, 2, 3..}, &spt);
     for (const auto &piece : spt.pieces()) {
        LOG(INFO) << piece.begin() << " " << piece.end();
     }
   #+END_SRC

   That looks pretty easy to deal with.

   CxxWrap.jl turns out to be indispensible here. I kept getting segfaults
   trying to write some wrapper functions marked =extern "C"...= and calling
   them with =ccall=. The same code "just worked" with CxxWrap, so I didn't go
   deep into debugging the first attempt.

   There is a little bit of nuissance around type conversions in
   CxxWrap. A C++ fn which returns a std::vector is wrapped so that that return
   value is auto cast into a julia Vector. But passing Julia vectors in does not
   autocast (I presume because it has to reallocate because julia values are
   values and C++ code could break that invariant, but I don't know. Making the
   paramters =const vector<int>= on the c++ side didn't help, but that was a
   long shot).

   But, CxxWrap does autocast Vector{Int32} into a C int*. It probably
   reallocates under the hood for the same reasons mentioned above.

   I'm currently using std::vector::assign to convert that array into a vector
   for sentencepiece, so we're probably allocating 3 times and copying twice,
   which will not fly if this needs to run in a hot spot.

   But profiling is a task for another day. For now, it works.
** [2023-04-11 Tue 09:22] Loading Local Modules
   Here's a gap in my understanding of Julia, or maybe just a mismatch between
   the way I approach writing programs and the assumptions of the language
   tooling:

   I tend to break large codebases up into many small files and group those
   heirarchically (modules, namespaces, whatever language X calls it).

   But I find myself fighting with =include= in Julia a lot. C style =include=
   should have died with C, in my opinion. There are just too many things that
   can go wrong when you copy-paste one file into another. Not to mention the
   perennial "where was *that* defined?" problem.

   So local modules it is. But the documentation is very sparse on how to work
   with local modules. The [[https://docs.julialang.org/en/v1/manual/modules/][modules]] documentation, unless I can't read, doesn't
   say anywhere how to get make sure the *code* of a module is loaded. You can't
   =import .MyModule= without an =include "./MyModule.jl"= it seems. The
   [[https://docs.julialang.org/en/v1/manual/code-loading/][Code Loading]] docs mention =LOAD_PATH= in passing in the preamble, but the
   forums and SO answers I've found all insist that one should use =Pkg= for all
   dependencies.

   The problem there is that I have modules with <30 lines of code, that have to
   be modules since they wrap shared libraries via FFI, and which cannot be
   =include=d because loading them is not idempotent. So far I've come up with 2
   solutions:

   #+BEGIN_SRC julia
     try
         Tokeniser
         @info "Tokeniser is already loaded, skipping"
     catch e
         if isa(e, UndefVarError)
             @info "Loading Tokeniser."
             include("./tokeniser.jl")
             import Main.Tokeniser as Tokeniser
         end
     end
   #+END_SRC

   which works but feels a bit clunky. I could write it as a macro easily
   enough, but it would still be a hack, plus I don't know how the module
   compiler cache would deal with this.

   The other is to add ="./"= to =LOAD_PATH=. This seems like the natural thing
   to do, especially given all of the talk about "package directories" in the
   code loading docs. But in those docs, there's no indication of how to point
   the resolver at a given "package directory"; no setting in =Project.toml=, no
   commands to =Pkg=, just some unspoken magic.

   My concern is that why is there no configuration for a package that says "in
   this package, sub packages are located at..."?

   The "right" way to do this seems to be to create a new git repo with a full
   blown package and use =Pkg.dev=. That's fine for large modules, but massive
   overkill when just playing around.

   I'm only writing this down so that I know why I've done what I have when a
   year from now I come back and say "What the hell was I thinking doing
   *that*?"
** [2023-04-11 Tue 10:45] Loading the Models
   I can't find any documentation on loading pytorch's .pth files without using
   pytorch. Even if I could, the binary can only be understood in the context of
   an out of band model (class) definition, so I don't think that's a great way
   to start.

   ggerganov has a tensor lib called [[https://github.com/ggerganov/ggml][ggml]] and a handy [[https://github.com/ggerganov/llama.cpp/blob/2663d2c6784ad7b77998c6874df25648d597f74b/convert-pth-to-ggml.py][script]] to convert pytorch
   binaries to his format, and his format is "documented" — which is to say the
   code is concise enough I can hope to read and understand it — so I can write
   a parser.

   N.B.: parsing binary is an infinitely finicky and brittle operation so I make
   no promises. The script I'm writing will probably work if you generated the
   ggml .bin files via the above script as of commit
   =f4f5362edb01b05c383b23f36d7b3489c77061b5=.

   I'll wrap it up in a script to make life easier.

   I'll immediately reexport to models into yet another format (but this time
   it's different!) so that I have something to work with.

   The field is still young and in flux, but wouldn't it be nice if we had
   standards for stuff like this? Think JSON for ml models. It's all just
   structs and arrays under the hood, whatever we call them and however much OO
   gobledygook we layer up on top.

   How much space do we save with these custom formats over, say json + deflate?
   What if we add in some primitive tags (#array{f16}, #end, I'm just making
   this up as I go along)? Think Armstrong's "Getting Erlang to Talk to the
   Outside World". Basically encode all machine words in LEB128, keep the 96
   ascii chars, and that leaves 31 tags for a state machine.

   #array #float "16"      <LEB128 encoded stream>
    ^ tag  ^ tag  ^ ascii    ^ Must be an even number of bytes in the end.

   As for tags, we need #array, #string, #bool, #float, #int, #uint, and the
   machinery to define structs (#struct NAME #struct-field FNAME #struct-val
   TYPE_TAG #end-struct) is a naive but sufficient set. NAME and FNAME are
   ascii strings (`"` delimited) and struct values types are just the type
   tags. The parser will have to be a state machine that expects a number after
   numeric tags #int <LEB encoded or ascii encoded "32"?>

   Lots of details to work out, but this isn't a new idea and it's worked
   before. The real issue is getting enough momentum that we don't end up
   creating just another pseudo standard.

   That's enough of that, back to work.
** [2023-04-13 Thu 08:59] Performance
   In the LLaMA paper, they claim to be able to process 380 tokens per second
   per 80GB A100 gpu for the 65B param model. If I can get within 1/100 of that
   with my setup it would be usable, if barely.

   Given that one cycle of loading the model on my hardware will take ~30s, this
   might just be a lost cause. But it will be fun to try and break it down so
   that it can run at all with less than half as much ram as needed to load the
   model and only 8GB vram
** [2023-04-16 Sun 12:12] swiGLU activation
   The llama paper states they use swiGLU activation in the feed forward layers,
   but there's no mention of the β parameter and it doesn't appear in the
   weights. The llama.cpp implementation uses silu (β = 1).

   The llama code uses =torch.nn.functional.silu= as well, so that seems to be
   the answer.
** [2023-04-16 Sun 13:11] Running the model
   Now that everything is loaded, let's see how far I can reason through the
   execution of it before I have to dig through the code.

   Normalisation:

   This seems simple enough. Zhang & Sennrich 2019: let ||.|| be the l² norm,
   then

   rms(g, x) = g .* x .* √length(x) ./ ||x||

   where g is the weight (gain) vector for the norm "layer" in the model dump.

   Looking at the python llama code, this looks to be what's happening.

   FFN layers:

   layers.i.feed_forward.w[1,2,3].weight are NxM, MxN, & NxM respectively, where
   N is the embedding size and M the hidden layer size.

   Based on Shazeer (2020), given input vector x the layer output should be

   (σ.(x * W) .* (x * V)) * W₂

   I'm guessing, from the sizes, that w1 is W, w2 is W₂, and w3 is V

   so we get

   ffn(x) = (σ.(x*w1) .* (x*w3)) * w2

   There don't seem to be any biases in the ggml model file. I'll need to check
   if they were elided or not part of the original llama model at all.

   Assuming the keyword param =bias=False= to the torch tensor constructors
   means what I think it means, there aren't any bias vectors to worry about.

   Attention:

   I still don't properly understand attention. Looking through Vaswani et
   al. 2017 I can't tell how the 4 matricies come in. For what they call "scaled
   dot product attemtion" we should get something like:

   softmax(wq*transpose(wk)./√d) * wv

   But where does the input come in? The paper makes it sound as if Q *is* the
   input (a batch of tokens encoded as a matrix), but it isn't clear enough to
   put in code. I'll have to read the implementations to understand the paper.

   Attention has always had the feel of magic sauce. Everyone writes just enough
   to brag to their peers, but never quite enough for an outsider to understand
   what's going on. Well, now I have code to read...
** [2023-04-17 Mon 17:53] Loading sharded models
   - Note taken on [2023-04-18 Tue 20:25] \\
     Additional observation: I'm getting errors when trying to save the entire model
     to bson and it appears that the BSON spec requires the number of bytes in the
     file to fit in an Int32. So saving 13GB files seems to be out of the question.
   - Note taken on [2023-04-18 Tue 20:09] \\
     Fun fact there: Flux.jl's recommended model saving method is the BSON.jl
     library, which sounds great, but it's way slower than reading/writing binary
     directly. I'm not sure why that is and it's very likely that I'm doing something
     wrong, but the docs make it out to be a one line solution to everything.
   I was avoiding thinking about this, but that was a mistake. The ggml format
   breaks up each tensor across each file rather than splitting the files by
   layers. I don't understand why this approach was chosen, though I'm sure they
   have their reasons.

   For my purposes though, it just adds another layer of nuissance. Now, in
   order to load a model that doesn't fit into ram, I have to iterate over each
   file for each tensor. That's not really a big deal, but it feels like an ugly
   way to do things. Not to mention the fact that the stitching back together of
   the pieces is done via implicit understanding which is brittle.

   This just means that I need to reserialise the models in my own format as
   soon as I know what that is so that I don't have to worry about 3rd party
   formats any longer.
** [2023-04-18 Tue 10:37] RoPE
   I ought to have read Su et al. 2022 before complaining that the attention
   mechanism is obscure, but it wouldn't have made a difference. After going
   through the paper, I'm still not much clearer on what attention heads do, nor
   which of the several formulae for attention are being used. There are a lot
   of underdefined variables (sim, ϕ, ψ) and I'm not sure what the relation is
   between the formulae for attention and the simple linear algebra equation:

   (Rᵈ_θ,m W_qxₘx)ᵀ(Rᵈ_θ,n Wₖxₙ)

   The math is straight forward enough, if quite tedious, but I'll need to go
   through the code with a comb to figure all of this out.

   The outer product form kind of looks like the code in =precompute_freqs_cis=,
   but there's something else going on.

   Name mapping: dim = embeddings, n_heads = attentionheads, and I think
   max_seq_len is contextwidth, but I'm not 100%.

   After reading through the papers again and staring at the code for god knows
   how long I think I understand what's going on here.

   let q = RxQ, k = RxK, and v = xV

   attn = softmax(qkᵀ./√d)V·O

   Where R is the block diagonal cis(nθ) matrix from Su et al. and the softmax
   is performed separately across each input and each head.

   The llama code does the rotation operation differently. It comes from the
   observation that

   (cosθ + isinθ)·(x₁ + x₂i) = x₁cosθ - x₂sinθ + i(x₂cosθ + x₁sinθ)

   which gives us the same result as the sum of the outer products with about
   half as much work.

   There's still the issue of the cache_k/v which I don't understand. That might
   just be an artefact of torch not being able to move lexical bindings into the
   gpu, but I'm not sure.
** [2023-04-28 Fri 11:12] Paying more attention
   I have a lot of assumptions to validate. The biggest one is that the data
   layout in ggml matches that expected by the llama python code. I'm putting
   that off because it's a doozy and if it's not true it might just be easier to
   write my own data exported from pth.

   Other confusing aspects: A(x) means xAᵀ upon reading the fairscale code and
   torch docs. Bloody OO. I'm starting to wonder if I'm interpreting any of the
   square matricies in the correct orientation.

   In the llama Attention class, xq, xk, & xv appear to be 3 tensors, despite
   the fact that as far as I can tell they ought to be plain matricies. My best
   guess at present is that the dimentions are batch, tokens, weights so that
   the first =transpose(1,2)= gets "the tokens to the outside" so that we have
   a #contextx#context matrix which conceptually weights each token by each
   other.

   why are V and O separate matricies? Ignoring the batch dimesion, the code
   just runs wo(scores*values) == scores*values*woᵀ. So why not precompute V*Oᵀ
   and not store both? Why spend the effort learning both?

   One glaring bit of ignorance: How in tarnation does this model produce more
   output tokens than there are tokens in the input? From what I've seen you can
   give it "once upon a time" and it will just spew text until the chips melt,
   but my current understanding does not accomodate that fact. Do you pad the
   input with zeros and then let it fill in the rest? And then fill another
   buffer half with the previous output and half with zeroes? The diagrams of
   transformers I've seen have encoder/decoder parallel components. Somehow I've
   lost that...
